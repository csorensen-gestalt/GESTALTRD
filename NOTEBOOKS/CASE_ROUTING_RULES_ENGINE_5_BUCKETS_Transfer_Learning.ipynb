{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the imports used in the program\n",
    "\n",
    "import pandas as pd \n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Bidirectional, LSTM\n",
    "from keras.layers import GlobalMaxPool1D, Conv1D, Dropout, GRU, Flatten, MaxPooling1D\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab data from a database\n",
    "\n",
    "def getData(Server, UID, PWD, Database, query):    \n",
    "    \n",
    "    #create a SQL connection based on the given server and database\n",
    "    sql_conn = pyodbc.connect('DRIVER={SQL Server};'\n",
    "                              'SERVER='+Server+';' \n",
    "                              'UID='+UID+';'\n",
    "                              'PWD='+PWD+';'\n",
    "                              'DATABASE='+Database+';' )\n",
    "    \n",
    "    #return the data from the given Query and SQL connection,\n",
    "    return pd.read_sql(query, sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish my server and corresponding database to pull data from\n",
    "server ='GSDEMO2HOST'\n",
    "database = 'MClinical'\n",
    "UID = 'gsanalytics'\n",
    "PWD = 'G3st@lt'\n",
    "\n",
    "\n",
    "#Stores the result in a pandas DataFrame object called data\n",
    "query =\"SELECT ISNULL(ProcedureStep.subSpecialtyCd,'UNKNOWN') AS subSpecialtyCd, FillerOrder.fillerOrderStatusCd, PL.locationName, BUCKETS1.BUCKETNUM FROM BUCKETS1 LEFT JOIN ProcedureStep ON BUCKETS1.PSKEY = ProcedureSteP.procedureStepKey LEFT JOIN RequestedProcedure ON ProcedureStep.requestedProcedureKey = RequestedProcedure.requestedProcedureKey LEFT JOIN FillerOrder ON RequestedProcedure.fillerOrderKey = FillerOrder.fillerOrderKey LEFT JOIN LOCATION AS PL ON FillerOrder.scheduledLocationKey = PL.locationKey\"\n",
    "original = getData(server, UID, PWD, database,query)\n",
    "data = original.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatinate the selected params\n",
    "def concatParam(df):\n",
    "    return  df['subSpecialtyCd'] + ' ' + df['fillerOrderStatusCd'] + ' ' + df['locationName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    #create a new column where its all the params concatinated by spaces\n",
    "    df['concat'] = concatParam(df)\n",
    "    \n",
    "    #change the case of all the words to lower case so there is no case sensitivity.\n",
    "    df['concat'] = df['concat'].str.lower()\n",
    "    return df['concat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['concat'] = clean(data)\n",
    "data.concat.apply(lambda x: len(x.split(\" \"))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text(df, maxlen, max_words):\n",
    "    #split df into two series\n",
    "    #texts being the concat\n",
    "    #labels being the cooresponding bucket\n",
    "    texts = df.concat\n",
    "    labels = df.BUCKETNUM\n",
    "    \n",
    "    #convert the series into numpy arrays\n",
    "    texts = texts.values\n",
    "    temp_labels = labels.values\n",
    "    \n",
    "    #create a empty array for our labels that we will convert to integers\n",
    "    labels = []\n",
    "    \n",
    "    #grab all the unique buckets (5) \n",
    "    #this will be our dictonary for mapping between integers and buckets\n",
    "    label_dict = [1,2,3,4,5]\n",
    "    label_dict = np.asarray(label_dict)\n",
    "\n",
    "    \n",
    "    for label_type in temp_labels:\n",
    "        labels.append(np.searchsorted(label_dict, label_type))\n",
    "    \n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    #create a tokenizer based on the max_words\n",
    "    #fit the tokenizer to our specific texts\n",
    "    #change our texts to a vetorized integer\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    #pad sequences ensures that all our vectors are of the same length\n",
    "    x = pad_sequences(sequences, maxlen=maxlen)\n",
    "    \n",
    "    \n",
    "\n",
    "    print('Shape of data tensor:', x.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "    \n",
    "    #return x, labels, and the last 7000 of x and labels for testing\n",
    "    return x, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 unique tokens.\n",
      "Shape of data tensor: (10, 8)\n",
      "Shape of label tensor: (10,)\n",
      "(10, 8)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "#define maxlen as the maximum words to take from each sectionValue\n",
    "#define max_words as the total number of unique words to tokenize\n",
    "\n",
    "maxlen = 8\n",
    "max_words = 20\n",
    "\n",
    "#create data that can be ran through our model\n",
    "x_test, y_test = convert_text(data, maxlen, max_words)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model on our set aside testing data\n",
    "def test_model(model, x_test):\n",
    "    #gather the models prediction \n",
    "    \n",
    "    #the model displays its prediction as a percent for every bucket at how confident the model is for \n",
    "    #each bucket. \n",
    "    \n",
    "    #the highest percent in our case the the bucket the model has choosen\n",
    "    preds = model.predict(x_test)\n",
    "    \n",
    "    y_pred = []\n",
    "    \n",
    "    #for ever row in the prediction list\n",
    "    #grab the max value and append that index to the y_pred\n",
    "    for row in preds:\n",
    "        y_pred.append(np.argmax(row))\n",
    "    \n",
    "    #convert the list to a numpy array\n",
    "    return np.asarray(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model against our test data and store the predictions in y_pred\n",
    "y_pred = test_model(model, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the confusion matrix\n",
    "def test_confusion_matrix(y_pred,y_test):\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the classification report\n",
    "def test_classification_report(y_pred,y_test):\n",
    "    print(skm.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 0 3]\n",
      " [6 0 0 0 0]\n",
      " [0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "test_confusion_matrix(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "           2       0.00      0.00      0.00       4.0\n",
      "           3       0.00      0.00      0.00       6.0\n",
      "           4       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00      10.0\n",
      "   macro avg       0.00      0.00      0.00      10.0\n",
      "weighted avg       0.00      0.00      0.00      10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display the classification report on the predictions\n",
    "test_classification_report(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop first n rows\n",
    "def drop_first_n_rows(df, n):\n",
    "    return df.iloc[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop frist N rows in data\n",
    "#WHERE N = #OF TRAIN DATA\n",
    "data = drop_first_n_rows(data, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the prediction number to bucket number\n",
    "def convert_to_bucket(Y):\n",
    "    #the prediction number is the index of the array 1,2,3,4,5 \n",
    "    #just add 1 to index to correct the bucket number\n",
    "    \n",
    "    #for every index in y\n",
    "    for row in range(0, len(Y)):\n",
    "        #replace the value at the index with value + 1\n",
    "        Y[row] = Y[row] + 1\n",
    "        \n",
    "    #return Y\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert pred\n",
    "pred = convert_to_bucket(y_pred)\n",
    "\n",
    "#convert test\n",
    "test = convert_to_bucket(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert our n x 1 numpy array to a pandas df\n",
    "pdpred = pd.DataFrame(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the pred list to the original dataframe\n",
    "\n",
    "data['pred'] = pdpred.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all the useless data\n",
    "\n",
    "data = data.drop(['subSpecialtyCd', 'fillerOrderStatusCd', 'locationName'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex the columns of the data\n",
    "data = data.reindex(columns=['concat', 'BUCKETNUM', 'pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concat</th>\n",
       "      <th>BUCKETNUM</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*gp f holy family hospital - incyte</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*gyp f incyte diagnostics walla walla</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*gyp f incyte diagnostics walla walla</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*gp f kadlec medical center</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*op f incyte diagnostics spokane</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>*gyp f incyte diagnostics walla walla</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>*gip f holy family hospital - incyte</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>*gyp f incyte diagnostics walla walla</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>*gyp f incyte diagnostics walla walla</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>*gyp f incyte diagnostics walla walla</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  concat  BUCKETNUM  pred\n",
       "0    *gp f holy family hospital - incyte          3     2\n",
       "1  *gyp f incyte diagnostics walla walla          4     1\n",
       "2  *gyp f incyte diagnostics walla walla          4     1\n",
       "3            *gp f kadlec medical center          3     5\n",
       "4       *op f incyte diagnostics spokane          3     5\n",
       "5  *gyp f incyte diagnostics walla walla          4     1\n",
       "6   *gip f holy family hospital - incyte          3     5\n",
       "7  *gyp f incyte diagnostics walla walla          4     1\n",
       "8  *gyp f incyte diagnostics walla walla          4     1\n",
       "9  *gyp f incyte diagnostics walla walla          4     1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the data\n",
    "data.head(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
