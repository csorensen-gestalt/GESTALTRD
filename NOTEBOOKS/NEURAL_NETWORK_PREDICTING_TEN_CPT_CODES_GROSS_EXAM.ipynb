{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#all the imports used in the program\n",
    "\n",
    "import pandas as pd \n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Bidirectional, LSTM\n",
    "from keras.layers import GlobalMaxPool1D, Conv1D, Dropout, GRU, Flatten, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab data from a database\n",
    "\n",
    "def getData(Server, UID, PWD, Database, query):    \n",
    "    \n",
    "    #create a SQL connection based on the given server and database\n",
    "    sql_conn = pyodbc.connect('DRIVER={SQL Server};'\n",
    "                              'SERVER='+Server+';' \n",
    "                              'UID='+UID+';'\n",
    "                              'PWD='+PWD+';'\n",
    "                              'DATABASE='+Database+';' )\n",
    "    \n",
    "    #return the data from the given Query and SQL connection,\n",
    "    #for other instances just simply change or remove depending on use\n",
    "    return pd.read_sql(query, sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish my server and corresponding database to pull data from\n",
    "server ='GSDEMO2HOST'\n",
    "database = 'MClinical'\n",
    "UID = 'gsanalytics'\n",
    "PWD = 'G3st@lt'\n",
    "\n",
    "#this query grabs sectionValues with their corresponding examCode and description\n",
    "#does not select examCodes if there is less than 100 section values for the corresponding examCode\n",
    "#Stores the result in a pandas DataFrame object called data\n",
    "query = \"SELECT CODEKEY.RESULTKEY, CODEKEY.CPT88304, CODEKEY.CPT88305, CODEKEY.CPT88307, CODEKEY.CPT88309, CODEKEY.CPT88331, CODEKEY.CPT88341, CODEKEY.CPT88342, CODEKEY.CPT88112, CODEKEY.CPT88141, CODEKEY.CPT88175, description, ResultSection.sectionValue FROM ResultSection LEFT JOIN CODEKEY ON ResultSection.resultKey = CODEKEY.resultkey LEFT JOIN mapResultRequestedProcedure ON ResultSection.resultKey  = mapResultRequestedProcedure.resultKey LEFT JOIN RequestedProcedure ON mapResultRequestedProcedure.requestedProcedureKey = RequestedProcedure.requestedProcedureKey LEFT JOIN FillerOrder ON RequestedProcedure.fillerOrderKey = FillerOrder.fillerOrderKey LEFT JOIN PlacerOrder ON FillerOrder.placerOrderKey = PlacerOrder.placerOrderKey LEFT JOIN ExamCode ON PlacerOrder.examCodeKey = ExamCode.examCodeKey WHERE sectionValue <> ' ' and ResultSection.sectionCategory like '%gross%' and description is not null and ResultSection.resultKey in ( select CodeKey.resultkey from CodeKey)\"\n",
    "\n",
    "\n",
    "original = getData(server, UID, PWD, database, query)\n",
    "data = original.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.sectionValue.values:\n",
    "    if(type(i) == float):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes words that have at one colon somewhere in the middle of two words\n",
    "#and words that have two colons separated by three words. \n",
    "\n",
    "def removeColon(df):\n",
    "    \n",
    "    #Word array of words that i will later remove\n",
    "    bagOfWords = []\n",
    "    \n",
    "    #a array of every word in the sectionValue on the given dataframe df\n",
    "    wordList = df.sectionValue.str.split(expand=True).stack()\n",
    "    \n",
    "    for word in wordList:\n",
    "        colonWord = re.search(r\"\\w+:\\w+:\\w+\", word)\n",
    "        if colonWord is None:\n",
    "            colonWord = re.search(r\"\\w+:\\w+\", word)\n",
    "        if colonWord is not None:\n",
    "            if colonWord.group() not in bagOfWords:\n",
    "                bagOfWords.append(colonWord.group())\n",
    "    \n",
    "    #return the updated dataframe sectionValue, only keeping words that are not contained in bagOfWords            \n",
    "    return df['sectionValue'].apply(lambda x: ' '.join([word for word in x.split() if word not in (bagOfWords)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is more useful than it looks.\n",
    "\n",
    "You pass in a pandas DataFrame and tweak it.\n",
    "\n",
    "First, i create a empty list called bagOfWords\n",
    "\n",
    "The next line seemes confusing but bassically what I am doing here is taking every word from the sectionValue column and creating a list in which each row only contains one word. This helps with the iterative process. I store the result of this into WordList(approx 1.4 million words)\n",
    "\n",
    "Next, i iterate through every word in the wordList in a for loop.\n",
    "\n",
    "let me explain how the search process works and what words i am looking to add to my bagOfWords\n",
    "1. How does the search processWork?\n",
    "    1. I use Regex(regular expression) to do my searching for me which is on a character by character basis\n",
    "2. What kind of words am i looking for?\n",
    "    1. \\w+:\\w+ and \\w+:\\w+:\\w+\n",
    "        1. \\w searches for any character in the form [a-zA-Z0-9]\n",
    "        2. \\+ searches for the previous search condition until the end of the word. \n",
    "        3. : specifies that i want a colon \n",
    "        4. putting it all together \n",
    "            1. \\w+:\\w+ searches for a character in the form [a-zA-Z0-9] for any amount of characters in that form until it hits a colon : in which then it does the same \\w+ until the end of the word.\n",
    "            2. \\w+:\\w+:\\w+ is the same as above just has two colons i hope you get the picture..\n",
    "            \n",
    "     \n",
    "Since i am searching for two different types of words i need to search two different times for every word in wordList.\n",
    "\n",
    "I search the word to see if it matches the pattern of having three words separated by 2 colons, this returns a match object which i store in colonWord.\n",
    "\n",
    "if the word isnt found in the search it returns None, so i check if colonWord is None. If it is i search for the different type of word and store that searches result into colonWord.\n",
    "\n",
    "After that process is done i finally check to see if either of my searches came back true(not None)\n",
    "\n",
    "If they do i use colonWord.group() function to grab just the string(word) that it found.\n",
    "\n",
    "Then Check the bagOfWords to see if the word i found is already in it. \n",
    "\n",
    "If the word is already in it I move onto the next word in the wordList.\n",
    "\n",
    "if it is not, i simply  add it and move to the next word as well. \n",
    "\n",
    "Finally once i have scanned all words and created my bagOfWords that is a unique list i remove those words from the sectionValue column of the Data. \n",
    "\n",
    "what the last line in the funtion is doing is recreating my column sectionValue, but only keeping words that are NOT in bagOfWords.\n",
    "\n",
    "Once that is done i return the new column of sectionValue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here i wanted to remove punctuation from the column sectionValue in my pandas dataFrame\n",
    "#i replace every character that matches with one of the following below with nothing.\n",
    "\n",
    "def removePunctuation(df1):\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace(',', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('.', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('?', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('/', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('/', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('+', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('-', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('=', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('_', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace(')', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('(', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('*', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('&', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('^', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('%', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('$', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('#', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('@', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('!', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('>', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('<', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('[', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace(']', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('{', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('}', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('|', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace(':', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace(';', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('\\'', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('\\\"', '')\n",
    "    return df1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(df):\n",
    "    \n",
    "    #stopWords are words that have relatively no meaning to any actual data\n",
    "    #we dont want that here so lets remove them\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    # add custom stopWords \n",
    "    stop = addStopWords(stop)\n",
    "    \n",
    "    # add this if you want to remove words that are smaller than size two\n",
    "    # change the size to whatever you like \n",
    "    \n",
    "    #df['sectionValue'] = df['sectionValue'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2]))\n",
    "    \n",
    "    #returning the new sectionValue to the Datafram with words that are not in the StopWords\n",
    "    return df['sectionValue'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding my own custom stopWords\n",
    "#super simple to add as you can see \n",
    "#modify as needed!\n",
    "\n",
    "def addStopWords(stop):\n",
    "    stop.append('-')\n",
    "    stop.append('a')\n",
    "    stop.append('b')\n",
    "    stop.append('c')\n",
    "    stop.append('d')\n",
    "    stop.append('e')\n",
    "    stop.append('f')\n",
    "    stop.append('g')\n",
    "    stop.append('h')\n",
    "    stop.append('i')\n",
    "    stop.append('j')\n",
    "    stop.append('k')\n",
    "    stop.append('l')\n",
    "    stop.append('m')\n",
    "    stop.append('n')\n",
    "    stop.append('o')\n",
    "    stop.append('p')\n",
    "    stop.append('q')\n",
    "    stop.append('r')\n",
    "    stop.append('s')\n",
    "    stop.append('t')\n",
    "    stop.append('u')\n",
    "    stop.append('v')\n",
    "    stop.append('w')\n",
    "    stop.append('x')\n",
    "    stop.append('y')\n",
    "    stop.append('z')\n",
    "    stop.append('no')\n",
    "    stop.append('see')\n",
    "    stop.append('two')\n",
    "    stop.append('0')\n",
    "    stop.append('1')\n",
    "    stop.append('2')\n",
    "    stop.append('3')\n",
    "    stop.append('4')\n",
    "    stop.append('5')\n",
    "    stop.append('6')\n",
    "    stop.append('7')\n",
    "    stop.append('8')\n",
    "    stop.append('9')\n",
    "    return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\csorensen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#clean_text transforms words like tomatoes, tomato, tomatos, all to tomato. this is very helpful.\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatinate the sectionValue and description feild\n",
    "def concatExamDesc(df):\n",
    "    return  df['description'] + ' ' + df['sectionValue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    #change the case of all the words to lower case so there is no case sensitivity.\n",
    "    df['sectionValue'] = df['sectionValue'].str.lower()\n",
    "\n",
    "    #call the removeColon function to remove words with a colon or mulitple colons in the middle of the word\n",
    "    df['sectionValue'] = removeColon(df)\n",
    "\n",
    "    #call the removePuncuation code, notice how i did this after the removeColon function.\n",
    "    #it is important that we call this after the removeColon Function because this would remove colons\n",
    "    #from words we want to remove, and then the remove colon function would never find anything because there is no colons. \n",
    "    df = removePunctuation(df)\n",
    "\n",
    "    #call the removeStopWords function to remove words that have no meaning.\n",
    "    df['sectionValue'] = removeStopWords(df)\n",
    "\n",
    "    #call the clean_text to place words of simularity with the base word (ex: biopsies -> biopsy)\n",
    "    df['sectionValue'] = df.sectionValue.apply(lambda x: clean_text(x))\n",
    "\n",
    "    #add the description to the sectionValue\n",
    "    df['sectionValue'] = concatExamDesc(df)\n",
    "    return df['sectionValue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103.76511226252158"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sectionValue'] = clean(data)\n",
    "data.sectionValue.apply(lambda x: len(x.split(\" \"))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text(df, maxlen, max_words):\n",
    "    #split df into two series\n",
    "    #texts being the sectionValue\n",
    "    #labels being the cooresponding examCode\n",
    "    texts = df.sectionValue\n",
    "    \n",
    "    #convert the series into numpy arrays\n",
    "    texts = texts.values\n",
    "    \n",
    "    #create a tokenizer based on the max_words\n",
    "    #fit the tokenizer to our specific texts\n",
    "    #change our texts to a vetorized integer\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    #pad sequences ensures that all our vectors are of the same length\n",
    "    x = pad_sequences(sequences, maxlen=maxlen)\n",
    "    \n",
    "    d = defaultdict(LabelEncoder)\n",
    "    \n",
    "    fit = df[['CPT88304', 'CPT88305', 'CPT88307',\n",
    "            'CPT88309', 'CPT88331', 'CPT88341', \n",
    "            'CPT88342', 'CPT88112', 'CPT88141', \n",
    "            'CPT88175']].apply(lambda y: d[y.name].fit_transform(y))   \n",
    "    labels = fit.values\n",
    "\n",
    "    print('Shape of data tensor:', x.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    #we need to randomize the indexs of our data because we had \n",
    "    #entered it all in ordely fashion. we dont want that. \n",
    "    #our model wouldnt learn correctly if it was ordered by examCode..\n",
    "    indices = np.arange(x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    x = x[indices]\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    #return x, labels, and the last 7000 of x and labels for testing\n",
    "    return x[:35000], labels[:35000], x[-35059:], labels[-35059:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23618 unique tokens.\n",
      "Shape of data tensor: (70059, 150)\n",
      "Shape of label tensor: (70059, 10)\n",
      "(35000, 150)\n",
      "(35000, 10)\n",
      "(35059, 150)\n",
      "(35059, 10)\n"
     ]
    }
   ],
   "source": [
    "#define maxlen as the maximum words to take from each sectionValue\n",
    "#define max_words as the total number of unique words to tokenize\n",
    "\n",
    "maxlen = 150\n",
    "max_words = 24000\n",
    "\n",
    "#create data that can be ran through our model\n",
    "x_train, y_train, x_test, y_test = convert_text(data, maxlen, max_words)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a machine learning model with the following\n",
    "def create_model(max_words,maxlen):\n",
    "    #keras default model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #add an embedding layer with the input dim and input length to what we have already\n",
    "    #configured for our vectorized forms of our text\n",
    "    model.add(Embedding(input_dim = max_words, input_length=maxlen, output_dim = 50))\n",
    "    \n",
    "    #model.add(Bidirectional(LSTM(64)))\n",
    "    \n",
    "    model.add(Dropout(0.15))\n",
    "    model.add(Conv1D(maxlen, 3, padding='valid', activation='relu', strides=1))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Conv1D(maxlen, 3, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    # create a dense output layer with the units = len(labels_dict)\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    \n",
    "    #print the summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\csorensen\\.conda\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\csorensen\\.conda\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 50)           1200000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 148, 150)          22650     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1510      \n",
      "=================================================================\n",
      "Total params: 1,224,160\n",
      "Trainable params: 1,224,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create the model\n",
    "model = create_model(max_words, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "def train_model(model, x_train, y_train, epochs, batchsize, max_words, max_len):\n",
    "    #compile the model\n",
    "    #optimizer -> Adam\n",
    "    #loss -> sparse categorical crossentropy (because we have a large multiclassifcation probelm)\n",
    "    #meteric -> accuracy\n",
    "    \n",
    "    #testing around with loss rate (LR)\n",
    "    \n",
    "    #notes, loss rate takes more epochs to train but helps the \n",
    "    #training curve stay closer to the validation curver for a lower loss/ higher accuracy\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=0.00009),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc']) \n",
    "    #save the history from the model\n",
    "    #set the paramiters\n",
    "    #fit the model \n",
    "    history = model.fit(x_train, \n",
    "                        y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batchsize,\n",
    "                        validation_split=0.2)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\csorensen\\.conda\\envs\\test\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\csorensen\\.conda\\envs\\test\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 28000 samples, validate on 7000 samples\n",
      "Epoch 1/75\n",
      "28000/28000 [==============================] - 29s 1ms/step - loss: 0.6847 - acc: 0.6363 - val_loss: 0.6551 - val_acc: 0.8184\n",
      "Epoch 2/75\n",
      "28000/28000 [==============================] - 30s 1ms/step - loss: 0.6260 - acc: 0.8714 - val_loss: 0.5901 - val_acc: 0.9186\n",
      "Epoch 3/75\n",
      "28000/28000 [==============================] - 28s 989us/step - loss: 0.5508 - acc: 0.9181 - val_loss: 0.5084 - val_acc: 0.9190\n",
      "Epoch 4/75\n",
      "28000/28000 [==============================] - 32s 1ms/step - loss: 0.4605 - acc: 0.9183 - val_loss: 0.4172 - val_acc: 0.9190\n",
      "Epoch 5/75\n",
      "28000/28000 [==============================] - 29s 1ms/step - loss: 0.3731 - acc: 0.9185 - val_loss: 0.3396 - val_acc: 0.9190\n",
      "Epoch 6/75\n",
      "28000/28000 [==============================] - 34s 1ms/step - loss: 0.3083 - acc: 0.9186 - val_loss: 0.2880 - val_acc: 0.9190\n",
      "Epoch 7/75\n",
      "28000/28000 [==============================] - 32s 1ms/step - loss: 0.2689 - acc: 0.9185 - val_loss: 0.2578 - val_acc: 0.9187\n",
      "Epoch 8/75\n",
      "28000/28000 [==============================] - 29s 1ms/step - loss: 0.2459 - acc: 0.9183 - val_loss: 0.2391 - val_acc: 0.9187\n",
      "Epoch 9/75\n",
      "28000/28000 [==============================] - 34s 1ms/step - loss: 0.2309 - acc: 0.9188 - val_loss: 0.2259 - val_acc: 0.9197\n",
      "Epoch 10/75\n",
      "28000/28000 [==============================] - 32s 1ms/step - loss: 0.2198 - acc: 0.9196 - val_loss: 0.2156 - val_acc: 0.9203\n",
      "Epoch 11/75\n",
      "28000/28000 [==============================] - 29s 1ms/step - loss: 0.2107 - acc: 0.9214 - val_loss: 0.2068 - val_acc: 0.9259\n",
      "Epoch 12/75\n",
      "28000/28000 [==============================] - 34s 1ms/step - loss: 0.2027 - acc: 0.9252 - val_loss: 0.1990 - val_acc: 0.9279\n",
      "Epoch 13/75\n",
      "28000/28000 [==============================] - 30s 1ms/step - loss: 0.1954 - acc: 0.9275 - val_loss: 0.1919 - val_acc: 0.9293\n",
      "Epoch 14/75\n",
      "28000/28000 [==============================] - 34s 1ms/step - loss: 0.1885 - acc: 0.9293 - val_loss: 0.1850 - val_acc: 0.9308\n",
      "Epoch 15/75\n",
      "28000/28000 [==============================] - 28s 989us/step - loss: 0.1818 - acc: 0.9310 - val_loss: 0.1782 - val_acc: 0.9326\n",
      "Epoch 16/75\n",
      "28000/28000 [==============================] - 33s 1ms/step - loss: 0.1750 - acc: 0.9330 - val_loss: 0.1712 - val_acc: 0.9365\n",
      "Epoch 17/75\n",
      "28000/28000 [==============================] - 30s 1ms/step - loss: 0.1680 - acc: 0.9371 - val_loss: 0.1641 - val_acc: 0.9393\n",
      "Epoch 18/75\n",
      "28000/28000 [==============================] - 34s 1ms/step - loss: 0.1610 - acc: 0.9397 - val_loss: 0.1570 - val_acc: 0.9411\n",
      "Epoch 19/75\n",
      "28000/28000 [==============================] - 37s 1ms/step - loss: 0.1539 - acc: 0.9417 - val_loss: 0.1500 - val_acc: 0.9444\n",
      "Epoch 20/75\n",
      "28000/28000 [==============================] - 33s 1ms/step - loss: 0.1471 - acc: 0.9447 - val_loss: 0.1434 - val_acc: 0.9473\n",
      "Epoch 21/75\n",
      "28000/28000 [==============================] - 30s 1ms/step - loss: 0.1408 - acc: 0.9470 - val_loss: 0.1373 - val_acc: 0.9492\n",
      "Epoch 22/75\n",
      "28000/28000 [==============================] - 36s 1ms/step - loss: 0.1351 - acc: 0.9492 - val_loss: 0.1317 - val_acc: 0.9512\n",
      "Epoch 23/75\n",
      "28000/28000 [==============================] - 33s 1ms/step - loss: 0.1299 - acc: 0.9511 - val_loss: 0.1267 - val_acc: 0.9537\n",
      "Epoch 24/75\n",
      "28000/28000 [==============================] - 29s 1ms/step - loss: 0.1251 - acc: 0.9533 - val_loss: 0.1221 - val_acc: 0.9562\n",
      "Epoch 25/75\n",
      "28000/28000 [==============================] - 36s 1ms/step - loss: 0.1207 - acc: 0.9552 - val_loss: 0.1180 - val_acc: 0.9583\n",
      "Epoch 26/75\n",
      "28000/28000 [==============================] - 28s 1ms/step - loss: 0.1167 - acc: 0.9576 - val_loss: 0.1141 - val_acc: 0.9597\n",
      "Epoch 27/75\n",
      "28000/28000 [==============================] - 36s 1ms/step - loss: 0.1132 - acc: 0.9592 - val_loss: 0.1105 - val_acc: 0.9607\n",
      "Epoch 28/75\n",
      "28000/28000 [==============================] - 27s 957us/step - loss: 0.1096 - acc: 0.9604 - val_loss: 0.1073 - val_acc: 0.9617\n",
      "Epoch 29/75\n",
      "28000/28000 [==============================] - 35s 1ms/step - loss: 0.1065 - acc: 0.9615 - val_loss: 0.1042 - val_acc: 0.9626\n",
      "Epoch 30/75\n",
      "28000/28000 [==============================] - 35s 1ms/step - loss: 0.1035 - acc: 0.9622 - val_loss: 0.1013 - val_acc: 0.9632\n",
      "Epoch 31/75\n",
      "28000/28000 [==============================] - 30s 1ms/step - loss: 0.1007 - acc: 0.9630 - val_loss: 0.0986 - val_acc: 0.9640\n",
      "Epoch 32/75\n",
      "28000/28000 [==============================] - 35s 1ms/step - loss: 0.0980 - acc: 0.9640 - val_loss: 0.0960 - val_acc: 0.9651\n",
      "Epoch 33/75\n",
      "28000/28000 [==============================] - 37s 1ms/step - loss: 0.0954 - acc: 0.9645 - val_loss: 0.0937 - val_acc: 0.9659\n",
      "Epoch 34/75\n",
      "28000/28000 [==============================] - 27s 976us/step - loss: 0.0930 - acc: 0.9655 - val_loss: 0.0914 - val_acc: 0.9670\n",
      "Epoch 35/75\n",
      "28000/28000 [==============================] - 35s 1ms/step - loss: 0.0908 - acc: 0.9665 - val_loss: 0.0893 - val_acc: 0.9679\n",
      "Epoch 36/75\n",
      "28000/28000 [==============================] - 28s 1ms/step - loss: 0.0888 - acc: 0.9672 - val_loss: 0.0873 - val_acc: 0.9687\n",
      "Epoch 37/75\n",
      "28000/28000 [==============================] - 35s 1ms/step - loss: 0.0869 - acc: 0.9680 - val_loss: 0.0856 - val_acc: 0.9695\n",
      "Epoch 38/75\n",
      "28000/28000 [==============================] - 28s 998us/step - loss: 0.0851 - acc: 0.9688 - val_loss: 0.0839 - val_acc: 0.9701\n",
      "Epoch 39/75\n",
      "28000/28000 [==============================] - 34s 1ms/step - loss: 0.0833 - acc: 0.9698 - val_loss: 0.0823 - val_acc: 0.9707\n",
      "Epoch 40/75\n",
      "28000/28000 [==============================] - 37s 1ms/step - loss: 0.0819 - acc: 0.9701 - val_loss: 0.0809 - val_acc: 0.9713\n",
      "Epoch 41/75\n",
      "28000/28000 [==============================] - 27s 954us/step - loss: 0.0804 - acc: 0.9707 - val_loss: 0.0796 - val_acc: 0.9717\n",
      "Epoch 42/75\n",
      "28000/28000 [==============================] - 35s 1ms/step - loss: 0.0791 - acc: 0.9714 - val_loss: 0.0784 - val_acc: 0.9723\n",
      "Epoch 43/75\n",
      "28000/28000 [==============================] - 36s 1ms/step - loss: 0.0780 - acc: 0.9719 - val_loss: 0.0773 - val_acc: 0.9725\n",
      "Epoch 44/75\n",
      "28000/28000 [==============================] - 29s 1ms/step - loss: 0.0768 - acc: 0.9724 - val_loss: 0.0764 - val_acc: 0.9730\n",
      "Epoch 45/75\n",
      "28000/28000 [==============================] - 35s 1ms/step - loss: 0.0758 - acc: 0.9730 - val_loss: 0.0755 - val_acc: 0.9736\n",
      "Epoch 46/75\n",
      "28000/28000 [==============================] - 31s 1ms/step - loss: 0.0748 - acc: 0.9733 - val_loss: 0.0746 - val_acc: 0.9739\n",
      "Epoch 47/75\n",
      "28000/28000 [==============================] - 30s 1ms/step - loss: 0.0739 - acc: 0.9738 - val_loss: 0.0738 - val_acc: 0.9744\n",
      "Epoch 48/75\n",
      "28000/28000 [==============================] - 35s 1ms/step - loss: 0.0731 - acc: 0.9742 - val_loss: 0.0731 - val_acc: 0.9747\n",
      "Epoch 49/75\n",
      "28000/28000 [==============================] - 34s 1ms/step - loss: 0.0722 - acc: 0.9743 - val_loss: 0.0725 - val_acc: 0.9750\n",
      "Epoch 50/75\n",
      "28000/28000 [==============================] - 29s 1ms/step - loss: 0.0714 - acc: 0.9746 - val_loss: 0.0718 - val_acc: 0.9751\n",
      "Epoch 51/75\n",
      "28000/28000 [==============================] - 35s 1ms/step - loss: 0.0708 - acc: 0.9749 - val_loss: 0.0712 - val_acc: 0.9754\n",
      "Epoch 52/75\n",
      "28000/28000 [==============================] - 31s 1ms/step - loss: 0.0701 - acc: 0.9751 - val_loss: 0.0708 - val_acc: 0.9757\n",
      "Epoch 53/75\n",
      "28000/28000 [==============================] - 30s 1ms/step - loss: 0.0695 - acc: 0.9755 - val_loss: 0.0702 - val_acc: 0.9758\n",
      "Epoch 54/75\n",
      "28000/28000 [==============================] - 36s 1ms/step - loss: 0.0689 - acc: 0.9755 - val_loss: 0.0697 - val_acc: 0.9760\n",
      "Epoch 55/75\n",
      "28000/28000 [==============================] - 27s 971us/step - loss: 0.0682 - acc: 0.9759 - val_loss: 0.0693 - val_acc: 0.9760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/75\n",
      "28000/28000 [==============================] - 33s 1ms/step - loss: 0.0675 - acc: 0.9761 - val_loss: 0.0689 - val_acc: 0.9761\n",
      "Epoch 57/75\n",
      "28000/28000 [==============================] - 29s 1ms/step - loss: 0.0670 - acc: 0.9762 - val_loss: 0.0685 - val_acc: 0.9761\n",
      "Epoch 58/75\n",
      "28000/28000 [==============================] - 35s 1ms/step - loss: 0.0665 - acc: 0.9765 - val_loss: 0.0681 - val_acc: 0.9762\n",
      "Epoch 59/75\n",
      "28000/28000 [==============================] - 28s 992us/step - loss: 0.0661 - acc: 0.9766 - val_loss: 0.0677 - val_acc: 0.9763\n",
      "Epoch 60/75\n",
      "28000/28000 [==============================] - 33s 1ms/step - loss: 0.0655 - acc: 0.9767 - val_loss: 0.0673 - val_acc: 0.9763\n",
      "Epoch 61/75\n",
      "28000/28000 [==============================] - 36s 1ms/step - loss: 0.0651 - acc: 0.9768 - val_loss: 0.0671 - val_acc: 0.9767\n",
      "Epoch 62/75\n",
      "28000/28000 [==============================] - 26s 940us/step - loss: 0.0646 - acc: 0.9771 - val_loss: 0.0667 - val_acc: 0.9767\n",
      "Epoch 63/75\n",
      "28000/28000 [==============================] - 35s 1ms/step - loss: 0.0641 - acc: 0.9771 - val_loss: 0.0664 - val_acc: 0.9768\n",
      "Epoch 64/75\n",
      "28000/28000 [==============================] - 35s 1ms/step - loss: 0.0636 - acc: 0.9774 - val_loss: 0.0660 - val_acc: 0.9768\n",
      "Epoch 65/75\n",
      "28000/28000 [==============================] - 28s 983us/step - loss: 0.0631 - acc: 0.9776 - val_loss: 0.0658 - val_acc: 0.9769\n",
      "Epoch 66/75\n",
      "28000/28000 [==============================] - 35s 1ms/step - loss: 0.0626 - acc: 0.9778 - val_loss: 0.0655 - val_acc: 0.9768\n",
      "Epoch 67/75\n",
      "28000/28000 [==============================] - 31s 1ms/step - loss: 0.0622 - acc: 0.9779 - val_loss: 0.0653 - val_acc: 0.9771\n",
      "Epoch 68/75\n",
      "28000/28000 [==============================] - 37s 1ms/step - loss: 0.0618 - acc: 0.9780 - val_loss: 0.0650 - val_acc: 0.9772\n",
      "Epoch 69/75\n",
      "28000/28000 [==============================] - 29s 1ms/step - loss: 0.0614 - acc: 0.9782 - val_loss: 0.0649 - val_acc: 0.9775\n",
      "Epoch 70/75\n",
      "28000/28000 [==============================] - 34s 1ms/step - loss: 0.0611 - acc: 0.9782 - val_loss: 0.0646 - val_acc: 0.9773\n",
      "Epoch 71/75\n",
      "28000/28000 [==============================] - 35s 1ms/step - loss: 0.0607 - acc: 0.9785 - val_loss: 0.0643 - val_acc: 0.9776\n",
      "Epoch 72/75\n",
      "28000/28000 [==============================] - 28s 999us/step - loss: 0.0603 - acc: 0.9787 - val_loss: 0.0641 - val_acc: 0.9776\n",
      "Epoch 73/75\n",
      "28000/28000 [==============================] - 36s 1ms/step - loss: 0.0599 - acc: 0.9787 - val_loss: 0.0640 - val_acc: 0.9778\n",
      "Epoch 74/75\n",
      "28000/28000 [==============================] - 34s 1ms/step - loss: 0.0596 - acc: 0.9789 - val_loss: 0.0637 - val_acc: 0.9778\n",
      "Epoch 75/75\n",
      "28000/28000 [==============================] - 27s 976us/step - loss: 0.0593 - acc: 0.9790 - val_loss: 0.0635 - val_acc: 0.9779\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "history = train_model(model, x_train, y_train, 75, 500, max_words, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the report for the training process\n",
    "def training_report(history):\n",
    "    #get the data from the model history file \n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    #set our epochs\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    #plot the accuracy \n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    \n",
    "    #plot the loss\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    #display the max values we attained\n",
    "    print('Validation Accuracy: ', val_acc[np.argmax(val_acc)] * 100)\n",
    "    print('Training Accuracy: ', acc[np.argmax(acc)] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  97.79287193502698\n",
      "Training Accuracy:  97.90465757250786\n"
     ]
    }
   ],
   "source": [
    "training_report(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model on our set aside testing data\n",
    "def test_model(model, x_test):\n",
    "    #gather the models prediction \n",
    "    \n",
    "    #the model displays its prediction as a list of all the cpt codes \n",
    "    #with percents in each category at how confident the model is for \n",
    "    #each cptCode. \n",
    "    \n",
    "    #since we used a binary classifier\n",
    "    #anything above .5 will  be considered true\n",
    "    #and anything below .5 will be considered false\n",
    "    preds = model.predict(x_test)\n",
    "    \n",
    "    #for ever row in the prediction list\n",
    "    #change every column value for the specific row\n",
    "    #where the percent is above or equal to .5 to 1 \n",
    "    #and below .5 to 0\n",
    "    preds[preds>=0.5] = 1\n",
    "    preds[preds<0.5] = 0\n",
    "    \n",
    "    #convert the list to a numpy array\n",
    "    return np.asarray(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model against our test data and store the predictions in y_pred\n",
    "y_pred = test_model(model, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the confusion matrix\n",
    "def test_confusion_matrix(y_pred,y_test):\n",
    "    matrix = skm.multilabel_confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    #create a label_dict\n",
    "    temp = ['88304', '88305', '88307',\n",
    "                '88309', '88331', '88341', \n",
    "                '88342', '88112', '88141', \n",
    "                '88175']\n",
    "    label_dict = np.asarray(temp)\n",
    "\n",
    "\n",
    "    #print the confusion matrix \n",
    "    #rows are the models predictions\n",
    "    #columns is the test\n",
    "\n",
    "    #since the matrix is of shape 10 x 2 x 2 \n",
    "    #we will go through each 2 x 2 matrix to display\n",
    "    #each cpt codes individual confusion matrix\n",
    "\n",
    "    x = 0\n",
    "    for i in matrix:\n",
    "        #convert matrix to a pandas df to convert\n",
    "        #index and columns to cpt codes\n",
    "        print(x)\n",
    "        dfmatrix = pd.DataFrame(i, columns=[0,label_dict[x]], index=[0,label_dict[x]])\n",
    "        print(dfmatrix, '\\n\\n')\n",
    "        x = x+1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the classification report\n",
    "def test_classification_report(y_pred,y_test):\n",
    "    #cr from index values\n",
    "    print(skm.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "           0  88304\n",
      "0      30836    160\n",
      "88304    774   3289 \n",
      "\n",
      "\n",
      "1\n",
      "          0  88305\n",
      "0      6804    833\n",
      "88305  1256  26166 \n",
      "\n",
      "\n",
      "2\n",
      "           0  88307\n",
      "0      28016    725\n",
      "88307    452   5866 \n",
      "\n",
      "\n",
      "3\n",
      "           0  88309\n",
      "0      33687    150\n",
      "88309    685    537 \n",
      "\n",
      "\n",
      "4\n",
      "           0  88331\n",
      "0      33405     39\n",
      "88331     79   1536 \n",
      "\n",
      "\n",
      "5\n",
      "           0  88341\n",
      "0      32577    393\n",
      "88341   1225    864 \n",
      "\n",
      "\n",
      "6\n",
      "           0  88342\n",
      "0      28702    611\n",
      "88342    984   4762 \n",
      "\n",
      "\n",
      "7\n",
      "           0  88112\n",
      "0      35058      0\n",
      "88112      1      0 \n",
      "\n",
      "\n",
      "8\n",
      "           0  88141\n",
      "0      35059      0\n",
      "88141      0      0 \n",
      "\n",
      "\n",
      "9\n",
      "           0  88175\n",
      "0      35059      0\n",
      "88175      0      0 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_confusion_matrix(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.88      4063\n",
      "           1       0.97      0.95      0.96     27422\n",
      "           2       0.89      0.93      0.91      6318\n",
      "           3       0.78      0.44      0.56      1222\n",
      "           4       0.98      0.95      0.96      1615\n",
      "           5       0.69      0.41      0.52      2089\n",
      "           6       0.89      0.83      0.86      5746\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.94      0.89      0.91     48476\n",
      "   macro avg       0.61      0.53      0.56     48476\n",
      "weighted avg       0.93      0.89      0.91     48476\n",
      " samples avg       0.96      0.93      0.94     48476\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csorensen\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\csorensen\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\csorensen\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#display the classification report on the predictions\n",
    "test_classification_report(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.976134516101429"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 -skm.hamming_loss(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
