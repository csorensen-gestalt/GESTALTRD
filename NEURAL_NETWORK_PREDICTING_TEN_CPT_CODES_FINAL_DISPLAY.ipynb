{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#all the imports used in the program\n",
    "\n",
    "import pandas as pd \n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Bidirectional, LSTM\n",
    "from keras.layers import GlobalMaxPool1D, Conv1D, Dropout, GRU, Flatten, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab data from a database\n",
    "\n",
    "def getData(Server, Database, query):    \n",
    "    \n",
    "    #create a SQL connection based on the given server and database\n",
    "    sql_conn = pyodbc.connect('DRIVER={SQL Server};'\n",
    "                          'SERVER='+Server+';' \n",
    "                          'DATABASE='+Database+';' \n",
    "                          'Trusted_Connection=yes')\n",
    "    \n",
    "    #return the data from the given Query and SQL connection,\n",
    "    #here i hard coded the index so all queries must select examCode\n",
    "    #for other instances just simply change or remove depending on use\n",
    "    return pd.read_sql(query, sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish my server and corresponding database to pull data from\n",
    "server ='GESTALT-BT41Q'\n",
    "database = 'MClinical'\n",
    "\n",
    "#this query grabs sectionValues with their corresponding examCode and description\n",
    "#does not select examCodes if there is less than 100 section values for the corresponding examCode\n",
    "#Stores the result in a pandas DataFrame object called data\n",
    "query = \"SELECT CPTCODEKEY.CPT88304, CPTCODEKEY.CPT88305, CPTCODEKEY.CPT88307, CPTCODEKEY.CPT88309, CPTCODEKEY.CPT88331, CPTCODEKEY.CPT88341, CPTCODEKEY.CPT88342, CPTCODEKEY.CPT88112, CPTCODEKEY.CPT88141, CPTCODEKEY.CPT88175, description, ResultSection.sectionValue FROM [MClinical].[dbo].[Result] LEFT JOIN ResultSection ON Result.resultKey = ResultSection.resultKey left join mapResultRequestedProcedure ON Result.resultKey = mapResultRequestedProcedure.resultKey  left join RequestedProcedure ON mapResultRequestedProcedure.requestedProcedureKey = RequestedProcedure.requestedProcedureKey left join FillerOrder ON RequestedProcedure.fillerOrderKey = FillerOrder.fillerOrderKey left join PlacerOrder ON FillerOrder.placerOrderKey = PlacerOrder.placerOrderKey left join ExamCode ON PlacerOrder.examCodeKey = ExamCode.examCodeKey left join Patient on FillerOrder.patientKey = Patient.patientKey left join cptcodekey on patient.patientkey = cptcodekey.patient_key WHERE sectionValue <> ' ' and (ResultSection.sectionValue <>' No diagnosis; performed technical only ') and ResultSection.sectionCategory like '%gross%' and examCode not like '%:%' and description is not null and Patient.patientKey in ( select patient_key from cptcodekey ) order by patient.patientkey\"\n",
    "\n",
    "\n",
    "\n",
    "original = getData(server,database,query)\n",
    "data = original.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes words that have at one colon somewhere in the middle of two words\n",
    "#and words that have two colons separated by three words. \n",
    "\n",
    "def removeColon(df):\n",
    "    \n",
    "    #Word array of words that i will later remove\n",
    "    bagOfWords = []\n",
    "    \n",
    "    #a array of every word in the sectionValue on the given dataframe df\n",
    "    wordList = df.sectionValue.str.split(expand=True).stack()\n",
    "    \n",
    "    for word in wordList:\n",
    "        colonWord = re.search(r\"\\w+:\\w+:\\w+\", word)\n",
    "        if colonWord is None:\n",
    "            colonWord = re.search(r\"\\w+:\\w+\", word)\n",
    "        if colonWord is not None:\n",
    "            if colonWord.group() not in bagOfWords:\n",
    "                bagOfWords.append(colonWord.group())\n",
    "    \n",
    "    #return the updated dataframe sectionValue, only keeping words that are not contained in bagOfWords            \n",
    "    return df['sectionValue'].apply(lambda x: ' '.join([word for word in x.split() if word not in (bagOfWords)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is more useful than it looks.\n",
    "\n",
    "You pass in a pandas DataFrame and tweak it.\n",
    "\n",
    "First, i create a empty list called bagOfWords\n",
    "\n",
    "The next line seemes confusing but bassically what I am doing here is taking every word from the sectionValue column and creating a list in which each row only contains one word. This helps with the iterative process. I store the result of this into WordList(approx 1.4 million words)\n",
    "\n",
    "Next, i iterate through every word in the wordList in a for loop.\n",
    "\n",
    "let me explain how the search process works and what words i am looking to add to my bagOfWords\n",
    "1. How does the search processWork?\n",
    "    1. I use Regex(regular expression) to do my searching for me which is on a character by character basis\n",
    "2. What kind of words am i looking for?\n",
    "    1. \\w+:\\w+ and \\w+:\\w+:\\w+\n",
    "        1. \\w searches for any character in the form [a-zA-Z0-9]\n",
    "        2. \\+ searches for the previous search condition until the end of the word. \n",
    "        3. : specifies that i want a colon \n",
    "        4. putting it all together \n",
    "            1. \\w+:\\w+ searches for a character in the form [a-zA-Z0-9] for any amount of characters in that form until it hits a colon : in which then it does the same \\w+ until the end of the word.\n",
    "            2. \\w+:\\w+:\\w+ is the same as above just has two colons i hope you get the picture..\n",
    "            \n",
    "     \n",
    "Since i am searching for two different types of words i need to search two different times for every word in wordList.\n",
    "\n",
    "I search the word to see if it matches the pattern of having three words separated by 2 colons, this returns a match object which i store in colonWord.\n",
    "\n",
    "if the word isnt found in the search it returns None, so i check if colonWord is None. If it is i search for the different type of word and store that searches result into colonWord.\n",
    "\n",
    "After that process is done i finally check to see if either of my searches came back true(not None)\n",
    "\n",
    "If they do i use colonWord.group() function to grab just the string(word) that it found.\n",
    "\n",
    "Then Check the bagOfWords to see if the word i found is already in it. \n",
    "\n",
    "If the word is already in it I move onto the next word in the wordList.\n",
    "\n",
    "if it is not, i simply  add it and move to the next word as well. \n",
    "\n",
    "Finally once i have scanned all words and created my bagOfWords that is a unique list i remove those words from the sectionValue column of the Data. \n",
    "\n",
    "what the last line in the funtion is doing is recreating my column sectionValue, but only keeping words that are NOT in bagOfWords.\n",
    "\n",
    "Once that is done i return the new column of sectionValue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here i wanted to remove punctuation from the column sectionValue in my pandas dataFrame\n",
    "#i replace every character that matches with one of the following below with nothing.\n",
    "\n",
    "def removePunctuation(df1):\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace(',', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('.', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('?', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('/', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('/', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('+', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('-', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('=', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('_', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace(')', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('(', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('*', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('&', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('^', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('%', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('$', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('#', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('@', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('!', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('>', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('<', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('[', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace(']', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('{', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('}', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('|', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace(':', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace(';', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('\\'', '')\n",
    "    df1['sectionValue'] = df1['sectionValue'].str.replace('\\\"', '')\n",
    "    return df1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(df):\n",
    "    \n",
    "    #stopWords are words that have relatively no meaning to any actual data\n",
    "    #we dont want that here so lets remove them\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    # add custom stopWords \n",
    "    stop = addStopWords(stop)\n",
    "    \n",
    "    # add this if you want to remove words that are smaller than size two\n",
    "    # change the size to whatever you like \n",
    "    \n",
    "    #df['sectionValue'] = df['sectionValue'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2]))\n",
    "    \n",
    "    #returning the new sectionValue to the Datafram with words that are not in the StopWords\n",
    "    return df['sectionValue'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding my own custom stopWords\n",
    "#super simple to add as you can see \n",
    "#modify as needed!\n",
    "\n",
    "def addStopWords(stop):\n",
    "    stop.append('-')\n",
    "    stop.append('a')\n",
    "    stop.append('b')\n",
    "    stop.append('c')\n",
    "    stop.append('d')\n",
    "    stop.append('e')\n",
    "    stop.append('f')\n",
    "    stop.append('g')\n",
    "    stop.append('h')\n",
    "    stop.append('i')\n",
    "    stop.append('j')\n",
    "    stop.append('k')\n",
    "    stop.append('l')\n",
    "    stop.append('m')\n",
    "    stop.append('n')\n",
    "    stop.append('o')\n",
    "    stop.append('p')\n",
    "    stop.append('q')\n",
    "    stop.append('r')\n",
    "    stop.append('s')\n",
    "    stop.append('t')\n",
    "    stop.append('u')\n",
    "    stop.append('v')\n",
    "    stop.append('w')\n",
    "    stop.append('x')\n",
    "    stop.append('y')\n",
    "    stop.append('z')\n",
    "    stop.append('no')\n",
    "    stop.append('see')\n",
    "    stop.append('two')\n",
    "    stop.append('0')\n",
    "    stop.append('1')\n",
    "    stop.append('2')\n",
    "    stop.append('3')\n",
    "    stop.append('4')\n",
    "    stop.append('5')\n",
    "    stop.append('6')\n",
    "    stop.append('7')\n",
    "    stop.append('8')\n",
    "    stop.append('9')\n",
    "    return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\csorensen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#clean_text transforms words like tomatoes, tomato, tomatos, all to tomato. this is very helpful.\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatinate the sectionValue and description feild\n",
    "def concatExamDesc(df):\n",
    "    return  df['description'] + ' ' + df['sectionValue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    #change the case of all the words to lower case so there is no case sensitivity.\n",
    "    df['sectionValue'] = df['sectionValue'].str.lower()\n",
    "\n",
    "    #call the removeColon function to remove words with a colon or mulitple colons in the middle of the word\n",
    "    df['sectionValue'] = removeColon(df)\n",
    "\n",
    "    #call the removePuncuation code, notice how i did this after the removeColon function.\n",
    "    #it is important that we call this after the removeColon Function because this would remove colons\n",
    "    #from words we want to remove, and then the remove colon function would never find anything because there is no colons. \n",
    "    df = removePunctuation(df)\n",
    "\n",
    "    #call the removeStopWords function to remove words that have no meaning.\n",
    "    df['sectionValue'] = removeStopWords(df)\n",
    "\n",
    "    #call the clean_text to place words of simularity with the base word (ex: biopsies -> biopsy)\n",
    "    df['sectionValue'] = df.sectionValue.apply(lambda x: clean_text(x))\n",
    "\n",
    "    #add the description to the sectionValue\n",
    "    df['sectionValue'] = concatExamDesc(df)\n",
    "    return df['sectionValue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the clean data function to clean data\n",
    "data['sectionValue'] = clean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text(df, maxlen, max_words):\n",
    "    #split df into two series\n",
    "    #texts being the sectionValue\n",
    "    #labels being the cooresponding examCode\n",
    "    texts = df.sectionValue\n",
    "    \n",
    "    #convert the series into numpy arrays\n",
    "    texts = texts.values\n",
    "    \n",
    "    #create a tokenizer based on the max_words\n",
    "    #fit the tokenizer to our specific texts\n",
    "    #change our texts to a vetorized integer\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    #pad sequences ensures that all our vectors are of the same length\n",
    "    x = pad_sequences(sequences, maxlen=maxlen)\n",
    "    \n",
    "    \n",
    "    #create a dictionary to map all\n",
    "    #cpt codes to 1 and keep other values as 0\n",
    "    d = defaultdict(LabelEncoder)\n",
    "    \n",
    "    fit = df[['CPT88304', 'CPT88305', 'CPT88307',\n",
    "            'CPT88309', 'CPT88331', 'CPT88341', \n",
    "            'CPT88342', 'CPT88112', 'CPT88141', \n",
    "            'CPT88175']].apply(lambda y: d[y.name].fit_transform(y))   \n",
    "    labels = fit.values\n",
    "\n",
    "    print('Shape of data tensor:', x.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "    \n",
    "    #return x, labels, and the last 7000 of x and labels for testing\n",
    "    return x[:13000], labels[:13000], x[-1419:], labels[-1419:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16999 unique tokens.\n",
      "Shape of data tensor: (14419, 150)\n",
      "Shape of label tensor: (14419, 10)\n",
      "(13000, 150)\n",
      "(13000, 10)\n",
      "(1419, 150)\n",
      "(1419, 10)\n"
     ]
    }
   ],
   "source": [
    "#define maxlen as the maximum words to take from each sectionValue\n",
    "#define max_words as the total number of unique words to tokenize\n",
    "\n",
    "maxlen = 150\n",
    "max_words = 20000\n",
    "\n",
    "#create data that can be ran through our model\n",
    "x_train, y_train, x_test, y_test = convert_text(data, maxlen, max_words)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a machine learning model with the following\n",
    "def create_model(max_words,maxlen):\n",
    "    #keras default model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #add an embedding layer with the input dim and input length to what we have already\n",
    "    #configured for our vectorized forms of our text\n",
    "    model.add(Embedding(input_dim = max_words, input_length=maxlen, output_dim = 50))\n",
    "    \n",
    "    #model.add(Bidirectional(LSTM(64)))\n",
    "    \n",
    "    model.add(Dropout(0.15))\n",
    "    model.add(Conv1D(maxlen, 3, padding='valid', activation='relu', strides=1))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Conv1D(maxlen, 3, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    # create a dense output layer with the units = len(labels_dict)\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    \n",
    "    #print the summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\csorensen\\.conda\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\csorensen\\.conda\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 50)           1000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 148, 150)          22650     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1510      \n",
      "=================================================================\n",
      "Total params: 1,024,160\n",
      "Trainable params: 1,024,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create the model\n",
    "model = create_model(max_words, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "def train_model(model, x_train, y_train, epochs, batchsize, max_words, max_len):\n",
    "    #compile the model\n",
    "    #optimizer -> adam (better for multi-label applications)\n",
    "    #loss -> sbinary_crossentropy(we are using this because each label is a unique binary case)\n",
    "    #meteric -> accuracy\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc']) \n",
    "    #save the history from the model\n",
    "    #set the paramiters\n",
    "    #fit the model \n",
    "    history = model.fit(x_train, \n",
    "                        y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batchsize)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\csorensen\\.conda\\envs\\test\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\csorensen\\.conda\\envs\\test\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/21\n",
      "13000/13000 [==============================] - 13s 1ms/step - loss: 0.5650 - acc: 0.8421\n",
      "Epoch 2/21\n",
      "13000/13000 [==============================] - 12s 898us/step - loss: 0.3171 - acc: 0.8938\n",
      "Epoch 3/21\n",
      "13000/13000 [==============================] - 12s 908us/step - loss: 0.2715 - acc: 0.9044\n",
      "Epoch 4/21\n",
      "13000/13000 [==============================] - 12s 929us/step - loss: 0.2531 - acc: 0.9093\n",
      "Epoch 5/21\n",
      "13000/13000 [==============================] - 12s 901us/step - loss: 0.2320 - acc: 0.9171\n",
      "Epoch 6/21\n",
      "13000/13000 [==============================] - 12s 909us/step - loss: 0.2086 - acc: 0.9263\n",
      "Epoch 7/21\n",
      "13000/13000 [==============================] - 13s 992us/step - loss: 0.1866 - acc: 0.9342\n",
      "Epoch 8/21\n",
      "13000/13000 [==============================] - 12s 899us/step - loss: 0.1713 - acc: 0.9403\n",
      "Epoch 9/21\n",
      "13000/13000 [==============================] - 12s 886us/step - loss: 0.1622 - acc: 0.9429\n",
      "Epoch 10/21\n",
      "13000/13000 [==============================] - 12s 918us/step - loss: 0.1559 - acc: 0.9454\n",
      "Epoch 11/21\n",
      "13000/13000 [==============================] - 12s 927us/step - loss: 0.1509 - acc: 0.9470\n",
      "Epoch 12/21\n",
      "13000/13000 [==============================] - 12s 942us/step - loss: 0.1459 - acc: 0.9492\n",
      "Epoch 13/21\n",
      "13000/13000 [==============================] - 12s 893us/step - loss: 0.1413 - acc: 0.9503\n",
      "Epoch 14/21\n",
      "13000/13000 [==============================] - 12s 916us/step - loss: 0.1364 - acc: 0.9512\n",
      "Epoch 15/21\n",
      "13000/13000 [==============================] - 13s 972us/step - loss: 0.1312 - acc: 0.9534\n",
      "Epoch 16/21\n",
      "13000/13000 [==============================] - 12s 893us/step - loss: 0.1255 - acc: 0.9556\n",
      "Epoch 17/21\n",
      "13000/13000 [==============================] - 12s 894us/step - loss: 0.1191 - acc: 0.9579\n",
      "Epoch 18/21\n",
      "13000/13000 [==============================] - 12s 902us/step - loss: 0.1123 - acc: 0.9608\n",
      "Epoch 19/21\n",
      "13000/13000 [==============================] - 12s 955us/step - loss: 0.1053 - acc: 0.9632\n",
      "Epoch 20/21\n",
      "13000/13000 [==============================] - 12s 902us/step - loss: 0.0985 - acc: 0.9658\n",
      "Epoch 21/21\n",
      "13000/13000 [==============================] - 13s 1ms/step - loss: 0.0919 - acc: 0.9681\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "history = train_model(model, x_train, y_train, 21, 500, max_words, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model on our set aside testing data\n",
    "def test_model(model, x_test):\n",
    "    #gather the models prediction \n",
    "    \n",
    "    #the model displays its prediction as a list of all the cpt codes \n",
    "    #with percents in each category at how confident the model is for \n",
    "    #each cptCode. \n",
    "    \n",
    "    #since we used a binary classifier\n",
    "    #anything above .5 will  be considered true\n",
    "    #and anything below .5 will be considered false\n",
    "    preds = model.predict(x_test)\n",
    "    \n",
    "    #for every row in the prediction list\n",
    "    #change every column value for the specific row\n",
    "    #where the percent is above or equal to .5 to 1 \n",
    "    #and below .5 to 0\n",
    "    preds[preds>=0.5] = 1\n",
    "    preds[preds<0.5] = 0\n",
    "    \n",
    "    #convert the list to a numpy array\n",
    "    return np.asarray(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy :  96.19450317124736\n"
     ]
    }
   ],
   "source": [
    "#test the model against our test data and store the predictions in y_pred\n",
    "y_pred = test_model(model, x_test)\n",
    "\n",
    "print('Test accuracy : ',(1 - skm.hamming_loss(y_test, y_pred)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop first n rows\n",
    "def drop_first_n_rows(df, n):\n",
    "    return df.iloc[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop frist 13000 rows in data\n",
    "data = drop_first_n_rows(data, 13000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 1's in columns to cooresponding cpt code\n",
    "def convert_to_cpt_code(Y):\n",
    "    #create a label dict\n",
    "    temp = ['88304', '88305', '88307',\n",
    "            '88309', '88331', '88341', \n",
    "            '88342', '88112', '88141', \n",
    "            '88175']\n",
    "    label_dict = np.asarray(temp)\n",
    "    \n",
    "    #create a container for the 1419 x 10 matrix (big storage container)\n",
    "    temp = []\n",
    "    \n",
    "    #access every row in Y\n",
    "    for row in Y:\n",
    "        #create a container for each row (mini storage container lol)\n",
    "        temp1 = []\n",
    "        i = 0\n",
    "        \n",
    "        #grab every column value in the cooresponding row\n",
    "        for index in row:\n",
    "            #if the value is 1 then append into our mini storage container\n",
    "            if (index == 1):\n",
    "                temp1.append(label_dict[i])\n",
    "            #append a 0    \n",
    "            else:\n",
    "                temp1.append('0')\n",
    "            \n",
    "            #increase the label dict tracker\n",
    "            i = i + 1\n",
    "            \n",
    "        #append the mini storage container to the big storage container\n",
    "        #NUMPY ARRAY !!!\n",
    "        temp.append(np.asarray(temp1))\n",
    "    \n",
    "    #numpy array!!!!!!\n",
    "    return np.asarray(temp)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = convert_to_cpt_code(y_pred)\n",
    "test = convert_to_cpt_code(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ['88304', '88305', '88307',\n",
    "        '88309', '88331', '88341', \n",
    "        '88342', '88112', '88141', \n",
    "        '88175']\n",
    "label_dict = np.asarray(temp)\n",
    "\n",
    "pdpred = pd.DataFrame(pred, columns=label_dict)\n",
    "pdtest = pd.DataFrame(test, columns=label_dict)\n",
    "\n",
    "#print(pdpred, '\\n\\n', pdtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_to_nan(df):\n",
    "    df['88304'].replace('0', np.nan, inplace=True)\n",
    "    df['88305'].replace('0', np.nan, inplace=True)\n",
    "    df['88307'].replace('0', np.nan, inplace=True)\n",
    "    df['88309'].replace('0', np.nan, inplace=True)\n",
    "    df['88331'].replace('0', np.nan, inplace=True)\n",
    "    df['88341'].replace('0', np.nan, inplace=True)\n",
    "    df['88342'].replace('0', np.nan, inplace=True)\n",
    "    df['88112'].replace('0', np.nan, inplace=True)\n",
    "    df['88141'].replace('0', np.nan, inplace=True)\n",
    "    df['88175'].replace('0', np.nan, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdpred = zero_to_nan(pdpred)\n",
    "pdtest = zero_to_nan(pdtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdtest['truth'] = pdtest[['88304', '88305', '88307', '88309', '88331', '88341', '88342', '88112', '88141', '88175']].apply(lambda x: ', '.join(x[x.notnull()]), axis = 1)\n",
    "pdpred['pred'] = pdpred[['88304', '88305', '88307', '88309', '88331', '88341', '88342', '88112', '88141', '88175']].apply(lambda x: ', '.join(x[x.notnull()]), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pred'] = pdpred['pred'].values\n",
    "data['truth'] = pdtest['truth'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['description', 'CPT88304', 'CPT88305', 'CPT88307',\n",
    "                  'CPT88309', 'CPT88331', 'CPT88341', \n",
    "                  'CPT88342', 'CPT88112', 'CPT88141', \n",
    "                  'CPT88175'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reindex(columns=['truth', 'pred', 'sectionValue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truth</th>\n",
       "      <th>pred</th>\n",
       "      <th>sectionValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13000</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. BREAST NEEDLE BIOPSY, RIGHT specimen receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13001</th>\n",
       "      <td>88304, 88305</td>\n",
       "      <td>88304</td>\n",
       "      <td>A. LIPOMA label designate clifford back receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13002</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. BREAST NEEDLE BIOPSY, RIGHT specimen label ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13003</th>\n",
       "      <td>88305, 88331</td>\n",
       "      <td>88305, 88331</td>\n",
       "      <td>A. SYNOVIUM specimen receive three part specim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13004</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305, 88342</td>\n",
       "      <td>A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13005</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305, 88342</td>\n",
       "      <td>A. SINUS CONTENTS specimen receive part specim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13006</th>\n",
       "      <td>88307, 88341, 88342</td>\n",
       "      <td>88307, 88342</td>\n",
       "      <td>A. LIVER NEEDLE BIOPSY specimen receive contai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13007</th>\n",
       "      <td>88304</td>\n",
       "      <td>88304</td>\n",
       "      <td>A. GALLBLADDER formalin label designate kitche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13008</th>\n",
       "      <td>88305, 88307, 88331, 88341, 88342</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. BREAST BIOPSY, WITHOUT SURGICAL MARGINS, LE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13009</th>\n",
       "      <td>88305, 88307, 88331, 88341, 88342</td>\n",
       "      <td>88305, 88307, 88309, 88331, 88341, 88342</td>\n",
       "      <td>A. LYMPH NODE, SENTINEL five specimen receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13010</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. BREAST NEEDLE BIOPSY, RIGHT specimen receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13011</th>\n",
       "      <td>88305, 88342</td>\n",
       "      <td>88305, 88342</td>\n",
       "      <td>A. DUODENAL BIOPSY receive part specimen label...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13012</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13013</th>\n",
       "      <td>88307</td>\n",
       "      <td>88307</td>\n",
       "      <td>A. UTERUS, OTHER THAN NEOPLASTIC/PROLAPSE spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13014</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. DUODENAL BIOPSY specimen receive five part ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13015</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. ENDOMETRIAL BIOPSY label designate sharp en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13016</th>\n",
       "      <td>88305, 88342</td>\n",
       "      <td>88305, 88342</td>\n",
       "      <td>A. DUODENAL BIOPSY specimen receive four part ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13017</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. SIGMOID POLYP specimen receive part labeled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13018</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. DUODENAL BIOPSY labeleddesignated espinoza ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13019</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13020</th>\n",
       "      <td>88305, 88342</td>\n",
       "      <td>88305, 88342</td>\n",
       "      <td>A. ANTRUM/ANTRAL BIOPSY specimen receive part ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13021</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. COLON BIOPSY specimen label designate white...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13022</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. RECTAL POLYP specimen label designate seay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13023</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. SIGMOID POLYP specimen receive part labeled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13024</th>\n",
       "      <td>88304</td>\n",
       "      <td>88304</td>\n",
       "      <td>A. SKIN, CYST/TAG/DEBRIDMENT , 1 BLOCK label s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13025</th>\n",
       "      <td>88304</td>\n",
       "      <td>88304</td>\n",
       "      <td>A. APPENDIX, OTHER THAN INCIDENTAL specimen la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13026</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. RECTUM specimen receive part specimen label...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13027</th>\n",
       "      <td>88307</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. ENDOMETRIAL CURETTINGS, 1 BLOCK specimen re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13028</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. ENDOMETRIAL BIOPSY label designate gomez em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13029</th>\n",
       "      <td>88307</td>\n",
       "      <td>88307</td>\n",
       "      <td>A. UTERUS, OTHER THAN NEOPLASTIC/PROLAPSE spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14389</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. ENDOMETRIAL BIOPSY specimen designate drill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14390</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14391</th>\n",
       "      <td>88305, 88342</td>\n",
       "      <td>88305, 88342</td>\n",
       "      <td>A. GASTRIC BIOPSY specimen label designate krz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14392</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14393</th>\n",
       "      <td>88305, 88331, 88341, 88342</td>\n",
       "      <td>88305, 88331, 88341, 88342</td>\n",
       "      <td>A. LUNG BIOPSY specimen receive fresh label co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14394</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. PRODUCTS OF CONCEPTION, SPONTANEOUS/MISSED ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14395</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. LABIAL BIOPSY specimen designate frey leave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14396</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. LARYNGEAL BIOPSY specimen label designate m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14397</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14398</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14399</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14400</th>\n",
       "      <td>88305, 88307, 88342</td>\n",
       "      <td>88305, 88331</td>\n",
       "      <td>A. TONGUE BIOPSY specimen receive fresh label ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14401</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. ENDOMETRIAL CURETTINGS, 1 BLOCK specimen de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14402</th>\n",
       "      <td>88307</td>\n",
       "      <td>88307</td>\n",
       "      <td>A. BREAST, PARTIAL MASTECTOMY, RIGHT specimen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14403</th>\n",
       "      <td>88304, 88305</td>\n",
       "      <td>88304, 88305</td>\n",
       "      <td>A. AORTIC VALVE specimen receive formalin spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14404</th>\n",
       "      <td>88307</td>\n",
       "      <td>88307</td>\n",
       "      <td>A. URINARY BLADDER CHIPS specimen designate be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14405</th>\n",
       "      <td>88305, 88342</td>\n",
       "      <td>88305, 88341, 88342</td>\n",
       "      <td>A. BREAST NEEDLE BIOPSY, RIGHT specimen design...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14406</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. ORAL MUCOSAL BIOPSY specimen designate mcgu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14407</th>\n",
       "      <td>88307</td>\n",
       "      <td>88307</td>\n",
       "      <td>A. THYROID, TOTAL specimen label designate mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14408</th>\n",
       "      <td>88304</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. CONJUCTIVAL BIOPSY specimen label designate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14409</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. TEMPORAL ARTERY BIOPSY specimen label desig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14410</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. BREAST NEEDLE BIOPSY, RIGHT specimen receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14411</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14412</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. FALLLOPIAN TUBE, ECTOPIC PREGNANCY, LEFT sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14413</th>\n",
       "      <td>88304</td>\n",
       "      <td>88304</td>\n",
       "      <td>A. GALLBLADDER specimen label designate layne ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14414</th>\n",
       "      <td>88304</td>\n",
       "      <td>88304</td>\n",
       "      <td>A. SKIN, CYST/TAG/DEBRIDMENT , 1 BLOCK specime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14415</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. JOINT, RESECTION specimen label designate h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14416</th>\n",
       "      <td>88307</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. EYE, RIGHT specimen label designate ouren r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14417</th>\n",
       "      <td>88307</td>\n",
       "      <td>88307</td>\n",
       "      <td>A. SMALL BOWEL, RESECTION OTHER THAN FOR TUMOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14418</th>\n",
       "      <td>88305</td>\n",
       "      <td>88305</td>\n",
       "      <td>A. TOE, AMPUTATION, NON-TRAUMATIC specimen lab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1419 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   truth  \\\n",
       "13000                              88305   \n",
       "13001                       88304, 88305   \n",
       "13002                              88305   \n",
       "13003                       88305, 88331   \n",
       "13004                              88305   \n",
       "13005                              88305   \n",
       "13006                88307, 88341, 88342   \n",
       "13007                              88304   \n",
       "13008  88305, 88307, 88331, 88341, 88342   \n",
       "13009  88305, 88307, 88331, 88341, 88342   \n",
       "13010                              88305   \n",
       "13011                       88305, 88342   \n",
       "13012                              88305   \n",
       "13013                              88307   \n",
       "13014                              88305   \n",
       "13015                              88305   \n",
       "13016                       88305, 88342   \n",
       "13017                              88305   \n",
       "13018                              88305   \n",
       "13019                              88305   \n",
       "13020                       88305, 88342   \n",
       "13021                              88305   \n",
       "13022                              88305   \n",
       "13023                              88305   \n",
       "13024                              88304   \n",
       "13025                              88304   \n",
       "13026                              88305   \n",
       "13027                              88307   \n",
       "13028                              88305   \n",
       "13029                              88307   \n",
       "...                                  ...   \n",
       "14389                              88305   \n",
       "14390                              88305   \n",
       "14391                       88305, 88342   \n",
       "14392                              88305   \n",
       "14393         88305, 88331, 88341, 88342   \n",
       "14394                              88305   \n",
       "14395                              88305   \n",
       "14396                              88305   \n",
       "14397                              88305   \n",
       "14398                              88305   \n",
       "14399                              88305   \n",
       "14400                88305, 88307, 88342   \n",
       "14401                              88305   \n",
       "14402                              88307   \n",
       "14403                       88304, 88305   \n",
       "14404                              88307   \n",
       "14405                       88305, 88342   \n",
       "14406                              88305   \n",
       "14407                              88307   \n",
       "14408                              88304   \n",
       "14409                              88305   \n",
       "14410                              88305   \n",
       "14411                              88305   \n",
       "14412                              88305   \n",
       "14413                              88304   \n",
       "14414                              88304   \n",
       "14415                              88305   \n",
       "14416                              88307   \n",
       "14417                              88307   \n",
       "14418                              88305   \n",
       "\n",
       "                                           pred  \\\n",
       "13000                                     88305   \n",
       "13001                                     88304   \n",
       "13002                                     88305   \n",
       "13003                              88305, 88331   \n",
       "13004                              88305, 88342   \n",
       "13005                              88305, 88342   \n",
       "13006                              88307, 88342   \n",
       "13007                                     88304   \n",
       "13008                                     88305   \n",
       "13009  88305, 88307, 88309, 88331, 88341, 88342   \n",
       "13010                                     88305   \n",
       "13011                              88305, 88342   \n",
       "13012                                     88305   \n",
       "13013                                     88307   \n",
       "13014                                     88305   \n",
       "13015                                     88305   \n",
       "13016                              88305, 88342   \n",
       "13017                                     88305   \n",
       "13018                                     88305   \n",
       "13019                                     88305   \n",
       "13020                              88305, 88342   \n",
       "13021                                     88305   \n",
       "13022                                     88305   \n",
       "13023                                     88305   \n",
       "13024                                     88304   \n",
       "13025                                     88304   \n",
       "13026                                     88305   \n",
       "13027                                     88305   \n",
       "13028                                     88305   \n",
       "13029                                     88307   \n",
       "...                                         ...   \n",
       "14389                                     88305   \n",
       "14390                                     88305   \n",
       "14391                              88305, 88342   \n",
       "14392                                     88305   \n",
       "14393                88305, 88331, 88341, 88342   \n",
       "14394                                     88305   \n",
       "14395                                     88305   \n",
       "14396                                     88305   \n",
       "14397                                     88305   \n",
       "14398                                     88305   \n",
       "14399                                     88305   \n",
       "14400                              88305, 88331   \n",
       "14401                                     88305   \n",
       "14402                                     88307   \n",
       "14403                              88304, 88305   \n",
       "14404                                     88307   \n",
       "14405                       88305, 88341, 88342   \n",
       "14406                                     88305   \n",
       "14407                                     88307   \n",
       "14408                                     88305   \n",
       "14409                                     88305   \n",
       "14410                                     88305   \n",
       "14411                                     88305   \n",
       "14412                                     88305   \n",
       "14413                                     88304   \n",
       "14414                                     88304   \n",
       "14415                                     88305   \n",
       "14416                                     88305   \n",
       "14417                                     88307   \n",
       "14418                                     88305   \n",
       "\n",
       "                                            sectionValue  \n",
       "13000  A. BREAST NEEDLE BIOPSY, RIGHT specimen receiv...  \n",
       "13001  A. LIPOMA label designate clifford back receiv...  \n",
       "13002  A. BREAST NEEDLE BIOPSY, RIGHT specimen label ...  \n",
       "13003  A. SYNOVIUM specimen receive three part specim...  \n",
       "13004  A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...  \n",
       "13005  A. SINUS CONTENTS specimen receive part specim...  \n",
       "13006  A. LIVER NEEDLE BIOPSY specimen receive contai...  \n",
       "13007  A. GALLBLADDER formalin label designate kitche...  \n",
       "13008  A. BREAST BIOPSY, WITHOUT SURGICAL MARGINS, LE...  \n",
       "13009  A. LYMPH NODE, SENTINEL five specimen receive ...  \n",
       "13010  A. BREAST NEEDLE BIOPSY, RIGHT specimen receiv...  \n",
       "13011  A. DUODENAL BIOPSY receive part specimen label...  \n",
       "13012  A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...  \n",
       "13013  A. UTERUS, OTHER THAN NEOPLASTIC/PROLAPSE spec...  \n",
       "13014  A. DUODENAL BIOPSY specimen receive five part ...  \n",
       "13015  A. ENDOMETRIAL BIOPSY label designate sharp en...  \n",
       "13016  A. DUODENAL BIOPSY specimen receive four part ...  \n",
       "13017  A. SIGMOID POLYP specimen receive part labeled...  \n",
       "13018  A. DUODENAL BIOPSY labeleddesignated espinoza ...  \n",
       "13019  A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...  \n",
       "13020  A. ANTRUM/ANTRAL BIOPSY specimen receive part ...  \n",
       "13021  A. COLON BIOPSY specimen label designate white...  \n",
       "13022  A. RECTAL POLYP specimen label designate seay ...  \n",
       "13023  A. SIGMOID POLYP specimen receive part labeled...  \n",
       "13024  A. SKIN, CYST/TAG/DEBRIDMENT , 1 BLOCK label s...  \n",
       "13025  A. APPENDIX, OTHER THAN INCIDENTAL specimen la...  \n",
       "13026  A. RECTUM specimen receive part specimen label...  \n",
       "13027  A. ENDOMETRIAL CURETTINGS, 1 BLOCK specimen re...  \n",
       "13028  A. ENDOMETRIAL BIOPSY label designate gomez em...  \n",
       "13029  A. UTERUS, OTHER THAN NEOPLASTIC/PROLAPSE spec...  \n",
       "...                                                  ...  \n",
       "14389  A. ENDOMETRIAL BIOPSY specimen designate drill...  \n",
       "14390  A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...  \n",
       "14391  A. GASTRIC BIOPSY specimen label designate krz...  \n",
       "14392  A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...  \n",
       "14393  A. LUNG BIOPSY specimen receive fresh label co...  \n",
       "14394  A. PRODUCTS OF CONCEPTION, SPONTANEOUS/MISSED ...  \n",
       "14395  A. LABIAL BIOPSY specimen designate frey leave...  \n",
       "14396  A. LARYNGEAL BIOPSY specimen label designate m...  \n",
       "14397  A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...  \n",
       "14398  A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...  \n",
       "14399  A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...  \n",
       "14400  A. TONGUE BIOPSY specimen receive fresh label ...  \n",
       "14401  A. ENDOMETRIAL CURETTINGS, 1 BLOCK specimen de...  \n",
       "14402  A. BREAST, PARTIAL MASTECTOMY, RIGHT specimen ...  \n",
       "14403  A. AORTIC VALVE specimen receive formalin spec...  \n",
       "14404  A. URINARY BLADDER CHIPS specimen designate be...  \n",
       "14405  A. BREAST NEEDLE BIOPSY, RIGHT specimen design...  \n",
       "14406  A. ORAL MUCOSAL BIOPSY specimen designate mcgu...  \n",
       "14407  A. THYROID, TOTAL specimen label designate mal...  \n",
       "14408  A. CONJUCTIVAL BIOPSY specimen label designate...  \n",
       "14409  A. TEMPORAL ARTERY BIOPSY specimen label desig...  \n",
       "14410  A. BREAST NEEDLE BIOPSY, RIGHT specimen receiv...  \n",
       "14411  A. SKIN, OTHER THAN CYST/TAG/DEBRIDEMENT/PLAST...  \n",
       "14412  A. FALLLOPIAN TUBE, ECTOPIC PREGNANCY, LEFT sp...  \n",
       "14413  A. GALLBLADDER specimen label designate layne ...  \n",
       "14414  A. SKIN, CYST/TAG/DEBRIDMENT , 1 BLOCK specime...  \n",
       "14415  A. JOINT, RESECTION specimen label designate h...  \n",
       "14416  A. EYE, RIGHT specimen label designate ouren r...  \n",
       "14417  A. SMALL BOWEL, RESECTION OTHER THAN FOR TUMOR...  \n",
       "14418  A. TOE, AMPUTATION, NON-TRAUMATIC specimen lab...  \n",
       "\n",
       "[1419 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsqlconn(Server, Database):    \n",
    "    \n",
    "    #create a SQL connection based on the given server and database\n",
    "    sql_conn = pyodbc.connect('DRIVER={SQL Server};'\n",
    "                          'SERVER='+Server+';' \n",
    "                          'DATABASE='+Database+';' \n",
    "                          'Trusted_Connection=yes')\n",
    "    \n",
    "    #return the data from the given Query and SQL connection,\n",
    "    #here i hard coded the index so all queries must select examCode\n",
    "    #for other instances just simply change or remove depending on use\n",
    "    return sql_conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "server ='GESTALT-BT41Q'\n",
    "database = 'CPTCode'\n",
    "\n",
    "connStr = getsqlconn(server,database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connStr.cursor()\n",
    "\n",
    "for index,row in data.iterrows():\n",
    "    cursor.execute(\"INSERT INTO dbo.MachineLearning([idx],[Pred],[Truth],[Val]) values (?,?, ?,?)\" ,index, row['pred'], row['truth'] , row['sectionValue']) \n",
    "    connStr.commit()\n",
    "cursor.close()\n",
    "connStr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
